<!doctype html><html lang="en-US"><!--  --><meta http-equiv="content-type" content="text/html;charset=UTF-8" /><!-- / -->
<head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1">  <link media="all" href="../../wp-content/cache/autoptimize/4/css/autoptimize_dc56928889f08ceb40a4520f8ece953b.css" rel="stylesheet" /><title>Find points whose pairwise distances approximate a given distance matrix &#8211; free-tor-game.com</title> <script>MathJax = {
  tex: {
    inlineMath: [['$','$'],['\\(','\\)']], 
    processEscapes: true
  },
  options: {
    ignoreHtmlClass: 'tex2jax_ignore|editor-rich-text'
  }
};</script> <link rel='dns-prefetch' href='http://cdn.jsdelivr.net/' /><link rel='dns-prefetch' href='http://cdnjs.cloudflare.com/' /><link rel='dns-prefetch' href='http://stackpath.bootstrapcdn.com/' /><link rel='dns-prefetch' href='http://s.w.org/' /><link rel='stylesheet' id='highlight-css'  href='../../../cdnjs.cloudflare.com/ajax/libs/highlight.js/11.2.0/styles/vs2015.min40df.css?ver=5.6' type='text/css' media='all' /><link rel='stylesheet' id='bootstrap-css'  href='../../../cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.1.3/css/bootstrap.mina352.css?ver=4.1.3' type='text/css' media='all' /><link rel='stylesheet' id='icons-css'  href='../../../stackpath.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min40df.css?ver=5.6' type='text/css' media='all' /> <script type='text/javascript' src='../../../cdnjs.cloudflare.com/ajax/libs/jquery/1.12.4/jquery.minb8ff.js?ver=1.12.4' id='jquery-js'></script> <script type='text/javascript' src='../../../cdnjs.cloudflare.com/ajax/libs/highlight.js/11.2.0/highlight.minc9a3.js?ver=11.2.0' id='highlight-js'></script>            <script>hljs.highlightAll();</script> <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-1731162805004860" crossorigin="anonymous"></script></head><body class="software-template-default single single-software postid-1075886 no-sidebar"><div id="page" class="site"> <a class="skip-link screen-reader-text" href="#content">Skip to content</a><header id="topnav" class="sticky-top"><nav class="navbar navbar-expand-md navbar-light bg-light" role="navigation"> <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbar-primary" aria-controls="navbar-primary" aria-expanded="false" aria-label="Toggle navigation"> <span class="navbar-toggler-icon"></span> </button> <a class="navbar-brand" href="#">free-tor-game.com</a><div id="navbar-primary" class="collapse navbar-collapse"><ul id="menu-primary-top-menu" class="nav navbar-nav"><li itemscope="itemscope" itemtype="https://www.schema.org/SiteNavigationElement" id="menu-item-192" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-has-children dropdown menu-item-192 nav-item"><a title="Tools" href="#" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false" class="dropdown-toggle nav-link" id="menu-item-dropdown-192">Tools</a><ul class="dropdown-menu" aria-labelledby="menu-item-dropdown-192" role="menu"><li itemscope="itemscope" itemtype="https://www.schema.org/SiteNavigationElement" id="menu-item-193" class="menu-item menu-item-type-post_type menu-item-object-post menu-item-193 nav-item"><a title="ARFCN-Frequency Converter" href="../../3gpp-arfcn-frequency-converter/" class="dropdown-item">ARFCN-Frequency Converter</a></li></ul></li><li itemscope="itemscope" itemtype="https://www.schema.org/SiteNavigationElement" id="menu-item-2301" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-2301 nav-item"><a title="Contact Us" href="../../contact/" class="nav-link">Contact Us</a></li></ul></div></nav></header><div id="content" class="site-content"><div id="primary" class="content-area"><main id="main" class="site-main"><div id="main-container" class="container"><div id="main-row" class="row"><div id="main-col" class="col-12"><article id="post-1075886" class="post-1075886 software type-software status-publish hentry category-software tag-algorithms tag-geometry"><div class="row"><div class="col-12 col-md-8"><header class="entry-header"><h1 class="entry-title text-danger">Find points whose pairwise distances approximate a given distance matrix</h1></header><p style='font-size:1.2em'><span class="mr-2 badge badge-success">algorithms</span><span class="mr-2 badge badge-info">geometry</span></p><div class="entry-content"><p><strong>Problem.</strong> I have a symmetric distance matrix with entries between zero and one, like this one:</p><pre><code>D = ( 0.0 0.4 0.0 0.5 )
    ( 0.4 0.0 0.2 1.0 )
    ( 0.0 0.2 0.0 0.7 )
    ( 0.5 1.0 0.7 0.0 )
</code></pre><p>I would like to find points in the plane that have (approximately) the pairwise distances given in D. I understand that this will usually not be possible with strictly correct distances, so I would be happy with a &#34;good&#34; approximation.</p><p>My matrices are smallish, no more than 10&#215;10, so performance is not an issue.</p><p><strong>Question.</strong> Does anyone know of an algorithm to do this?</p><p><strong>Background.</strong> I have sets of probability densities between which I calculate <a href="https://en.wikipedia.org/wiki/Hellinger_distance" rel="nofollow noreferrer">Hellinger distances</a>, which I would like to visualize as above. Each set contains no more than 10 densities (see above), but I have a couple of hundred sets.</p><p><strong>What I did so far.</strong></p><ul><li>I did consider posting at <a href="https://math.stackexchange.com/questions">math.SE</a>, but looking at what gets <a href="https://math.stackexchange.com/questions/tagged/geometry">tagged as &#34;geometry&#34;</a> there, it seems like this kind of computational geometry question would be more on-topic here. If the community thinks this should be migrated, please go ahead.</li><li>This looks like a straightforward problem in computational geometry, and I would assume that anyone involved in clustering might be interested in such a visualization, but I haven&#39;t been able to google anything.</li><li>One simple approach would be to randomly plonk down points and perturb them until the distance matrix is close to D, e.g., using Simulated Annealing, or run a Genetic Algorithm. I have to admit that I haven&#39;t tried that yet, hoping for a smarter way.</li><li>One specific operationalization of a &#34;good&#34; approximation in the sense above is Problem 4 in the Open Problems section <a href="http://libflow.com/d/zmkydw0x/Computational_Geometry" rel="nofollow noreferrer">here</a>, with k=2. Now, while finding an algorithm that is <em>guaranteed</em> to find the minimum l1-distance between D and the resulting distance matrix may be an open question, it still seems possible that there at least is some approximation to this optimal solution. If I don&#39;t get an answer here, I&#39;ll mail <a href="https://www.cs.utah.edu/~suresh/web/" rel="nofollow noreferrer">the gentleman who posed that problem</a> and ask whether he knows of any approximation algorithm (and post any answer I get to that here).</li></ul></div>     <div id="comments" class="comments-area"><div class="row"><div class="col-12"><div class="mt-3 border-bottom border-success"><h4 class="text-success"><i class='fa fa-check-circle text-success mr-3'></i><span>Best Answer</span></h4></div><div class='bg-transparent mb-3'><p>This is the standard problem known as &#34;<a href="http://en.wikipedia.org/wiki/Multidimensional_scaling" rel="nofollow noreferrer">Multidimensional Scaling</a>&#34;.</p><p>There are lots of varieties, so I recommend reading the wikipedia page on the subject (it&#39;s not so long), but in your case, it looks like plain PCA (<a href="http://en.wikipedia.org/wiki/Principal_component_analysis" rel="nofollow noreferrer">Principal component analysis</a>) is the way to go.</p><p>In particular, a (squared) distance matrix <strong>D</strong> is closely related to <strong>B</strong>, the product of the transposed position matrix <strong>X</strong> with itself:</p><p><strong><code>B = -0.5 * J D J</code></strong> where <strong><code>J = I - 1/n</code></strong> (i.e. 1-1/n on the diagonal, -1/n elsewhere)</p><p><strong><code>B = X^T X</code></strong> (by construction)</p><p>PCA will give you the eigenvalues and eigenvectors such that</p><p><strong><code>B = U^T L U</code></strong></p><p>with the columns of U being the eigenvectors, and the diagonal of L being the eigenvalues.</p><p>Then it&#39;s just a question of multiplication:</p><p><strong><code>X = L^(1/2) U</code></strong></p><p>If you&#39;re not used to math this may sound a little scary, but PCA is almost certainly available as a library whatever platform you&#39;re on, and then it&#39;s just a question of a multiplying a diagonal matrix with matrix, which is easy to do manually if necessary.  PCA will reconstruct a full-dimensional space, so if you want just the 2 or 3 most significant dimensions, simply crop the resultant matrix.  More significant dimensions are those with the highest eigenvalue; most PCA algorithms list them in order, so you can just take the top few rows.</p><p>There are various documents online that explain this in more detail; alas wikipedia falls short here.  See, for instance: <a href="http://www.mathpsy.uni-tuebingen.de/wickelmaier/pubs/Wickelmaier2003SQRU.pdf" rel="nofollow noreferrer">Wickelmaier, F. (2003). An introduction to MDS. Reports from the Sound Quality Research Unit (SQRU), No. 7</a> pages 9-11 for more info.</p><hr/><p>The above is a general pattern you might apply in many languages.  To verify this works, I tested it using <a href="http://eigen.tuxfamily.org/" rel="nofollow noreferrer">Eigen</a> which is the fastest linear algebra library I know of that can express this kind of thing simply.</p><p>I&#39;ll walk you through it:</p><pre><code>#include &lt;iostream&gt;
#include &lt;Eigen/Core&gt;
#include &lt;Eigen/Eigenvalues&gt; 

using namespace Eigen;
using namespace std;

#define DBG(s) (cout&lt;&lt; #s&lt;&lt;&#34;:\n&#34;&lt;&lt;(s)&lt;&lt;&#34;\n\n&#34; )


int main(int argc, char* argv[])
{
</code></pre><p>The above is just boilerplate referring to <a href="http://eigen.tuxfamily.org/" rel="nofollow noreferrer">Eigen</a> and simple console output function.</p><pre><code>    auto const n = 4;//4 points.

    MatrixXd points(3, n);
    points &lt;&lt;
        0.0,   4.0,  2.0,   3.0,
        3.0,   1.0,  0.0,  -4.0,
        0.01, -0.1, -0.05, -0.01;


    MatrixXd D(n, n);
    for (int i = 0; i &lt; n; i++)
        D.col(i) = (points.colwise() - points.col(i)).colwise().squaredNorm().transpose();

    DBG(D);
    //      0 20.0121 13.0036 58.0004
    //20.0121       0  5.0025 26.0081
    //13.0036  5.0025       0 17.0016
    //58.0004 26.0081 17.0016       0
</code></pre><p>I&#39;m initializing the input (squared) distance matrix <em>D</em> with real distances to make sure they sort of make sense.  Notice that the third dimensions values are all really small - I intend to make a 2D reconstruction and I&#39;d like data that&#39;s not quite perfect :-).</p><pre><code>    MatrixXd J = MatrixXd::Identity(n, n) - MatrixXd::Constant(n, n, 1.0 / 4.0);
    DBG(J);
    // 0.75 -0.25 -0.25 -0.25
    //-0.25  0.75 -0.25 -0.25
    //-0.25 -0.25  0.75 -0.25
    //-0.25 -0.25 -0.25  0.75

    MatrixXd B = -0.5 * J * D * J;
    DBG(B);
    //  14.0648 -0.940469  0.561906  -13.6862
    //-0.940469   4.06641 -0.436719  -2.68922
    // 0.561906 -0.436719 0.0626563 -0.187844
    // -13.6862  -2.68922 -0.187844   16.5633
</code></pre><p>Note that <em>B</em> has a non-zero diagonal (you worried about this in the comments below).</p><pre><code>    SelfAdjointEigenSolver&lt;MatrixXd&gt; eigendecomp(B);
    DBG(eigendecomp.eigenvalues());
    //-1.77725e-015
    //  0.000568306
    //      5.61749
    //       29.139
</code></pre><p>Note that Eigen returns eigenvalues in ascending order; also note that the first eigenvalue is negative.  This is a problem, because we&#39;re going to take the square root.  Of course, this is just a numeric artifact; since our input data was 3D, it makes sense that the least-significant dimention of a 4D representation is just noise.  It&#39;s tiny too.  In general, you need to make sure that you pick the right number of dimensions to use, because too few and you&#39;ll introduce error; too many and you&#39;ll run into negative eigenvalues.  Note that if any of these negative eigenvalues were <strong>large</strong>, that would indicate your input is bad, such as that it violates the triangle inequality that A to C via B is always longer than A to C directly.</p><p>Your input above has such a violation: points 0 and 2 are the same (have 0 distance), yet point 0 is at distance 0.4 from point 1, but point 2 is at distance 0.2.  Clearly regardless of dimensionality or where you put the points, this is impossible.  That&#39;s going to cause negative eigenvalues that aren&#39;t just numerical noise.  There are various hacks to mitigate that, but let&#39;s get back on track and use those eigenvalues:</p><pre><code>    MatrixXd X = eigendecomp.eigenvalues().bottomRows(2).cwiseSqrt().asDiagonal()
        * eigendecomp.eigenvectors().transpose().bottomRows(2);
</code></pre><p>That&#39;s just the wordy way of saying <strong><code>X = L^(1/2) U</code></strong> while only considering the most significant 2 dimensions.</p><pre><code>    DBG(X.transpose() * X);
    //  14.0647 -0.940508  0.562078  -13.6863
    //-0.940508   4.06638 -0.436623  -2.68925
    // 0.562078 -0.436623 0.0622356 -0.187691
    // -13.6863  -2.68925 -0.187691   16.5632
</code></pre><p>Right, so we can verify that the decomposition works: this is quite similar to <em>B</em>. It&#39;s not exactly the same because we chopped off two dimensions, after all.</p><pre><code>    DBG(X);
    //-0.999577   1.99533 -0.232164 -0.763594
    //  3.61463  0.291588 0.0912992  -3.99751
</code></pre><p>These points turn out not to be very similar to the original points we started from.  That&#39;s not actually surprising; after all, distances are invariant under rotation and translation, so you&#39;ll find that <em>these</em> points are zero-centered, and they&#39;re arbitrarily rotated compared to the points we started with.  You could actually reconstruct the rotation matrix effectively used with another PCA, but let&#39;s not :-).</p><pre><code>    MatrixXd D2(n, n);
    for (int i = 0; i &lt; n; i++)
        D2.col(i) = (X.colwise() - X.col(i)).colwise().squaredNorm().transpose();
    DBG(D2);
    //      0 20.0121 13.0028 58.0004
    //20.0121       0 5.00187 26.0081
    //13.0028 5.00187       0 17.0008
    //58.0004 26.0081 17.0008       0

    return 0;
}
</code></pre><p>As a final verification, note that the distances these new points generate are pretty much identical to the distances we started with.  Numerical inaccuracies and more importantly our 3D -&gt; 2D reduction mean it&#39;s not completely perfect, but you should be able to see that <strong>D</strong> is almost the same as <strong>D2</strong>.</p><hr/><p><strong>Note for when your data really isn&#39;t very clean and/or doesn&#39;t actually represent Euclidean distances:</strong> MDS (using PCA) solves the optimization problem of minimizing the squared error of the distance.  That&#39;s all fine and dandy, but notice that errors in large distances are (over-)emphasized here (1<sup>2</sup> - 0<sup>2</sup> = 1, but 11<sup>2</sup> - 10<sup>2</sup> = 21, so MDS will try 21 times as hard to fix the second error).  If your distances aren&#39;t perfect, PCA will try to make the &#34;most significant&#34; i.e. largest distance fit best.  This is often actually exactly the <em>wrong</em> thing to do; usually, you care more about local structure than global structure. <strong>IF</strong> your distance matrix is inaccurate and you cannot reconstruct a good overall position matrix, you might want to look at techniques that at least reconstruct local neighbourhoods best.  State of the art there (AFAIK) is <strong><a href="http://homepage.tudelft.nl/19j49/t-SNE.html" rel="nofollow noreferrer">t-SNE</a></strong>: you won&#39;t get an absolutely accurate map from that technique, but you will get the right things close to each other, at least.</p><hr/><p>Good luck - sounds like a fun problem!</p></div></div><div class="col-4"></div></div></div></div><div class="col-12 col-md-4"> <ins class="adsbygoogle"
 style="display:block"
 data-ad-client="ca-pub-1731162805004860"
 data-ad-slot="1340983829"
 data-ad-format="auto"
 data-full-width-responsive="true"></ins>   <div class='mt-3 ml-4 border-bottom border-success'><h6><span>Related Question</span></h6></div><ul class='list-group list-group-flush'><li class="list-group-item"><a href='../find-all-points-within-a-certain-distance-of-each-other/'>Find all points within a certain distance of each other</a></li><li class="list-group-item"><a href='../finding-the-lowest-average-hamming-distance-when-the-order-of-the-strings-matter/'>Finding the lowest average Hamming distance when the order of the strings matter</a></li><li class="list-group-item"><a href='../find-a-line-that-is-closest-to-scattered-points/'>Find a line that is closest to scattered points</a></li><li class="list-group-item"><a href='../how-to-efficiently-search-a-set-of-vectors-for-a-vector-which-is-the-closest-match/'>How to efficiently search a set of vectors for a vector which is the closest match</a></li></ul></div></div><footer class="entry-footer"></footer></article></div></div></div></main></div></div><footer id="colophon" class="site-footer"><div class="site-info"></div></footer></div>    <script type='text/javascript' src='../../../cdn.jsdelivr.net/npm/mathjax%403/es5/tex-chtml40df.js?ver=5.6' id='mathjax-js'></script> <script type='text/javascript' src='../../../cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/esm/popper.min43d3.js?ver=1.14.7' id='popper-js'></script> <script type='text/javascript' src='../../../cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.3.1/js/bootstrap.min5b31.js?ver=4.3.1' id='bootstrap4-js'></script> <script type='text/javascript' id='itectec-common-js-extra'>var globalObject = {"homeUrl":"https:\/\/free-tor-game.com\/"};</script> <script defer src="../../wp-content/cache/autoptimize/4/js/autoptimize_63b39f43631847816e6b0e70ea726624.js"></script></body></html>