<!doctype html><html lang="en-US"><!--  --><meta http-equiv="content-type" content="text/html;charset=UTF-8" /><!-- / -->
<head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1">  <link media="all" href="../../wp-content/cache/autoptimize/4/css/autoptimize_dc56928889f08ceb40a4520f8ece953b.css" rel="stylesheet" /><title>Approaching Particle Swarm Optimization with optimization problem, how to use PSO with it &#8211; free-tor-game.com</title> <script>MathJax = {
  tex: {
    inlineMath: [['$','$'],['\\(','\\)']], 
    processEscapes: true
  },
  options: {
    ignoreHtmlClass: 'tex2jax_ignore|editor-rich-text'
  }
};</script> <link rel='dns-prefetch' href='http://cdn.jsdelivr.net/' /><link rel='dns-prefetch' href='http://cdnjs.cloudflare.com/' /><link rel='dns-prefetch' href='http://stackpath.bootstrapcdn.com/' /><link rel='dns-prefetch' href='http://s.w.org/' /><link rel='stylesheet' id='highlight-css'  href='../../../cdnjs.cloudflare.com/ajax/libs/highlight.js/11.2.0/styles/vs2015.min40df.css?ver=5.6' type='text/css' media='all' /><link rel='stylesheet' id='bootstrap-css'  href='../../../cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.1.3/css/bootstrap.mina352.css?ver=4.1.3' type='text/css' media='all' /><link rel='stylesheet' id='icons-css'  href='../../../stackpath.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min40df.css?ver=5.6' type='text/css' media='all' /> <script type='text/javascript' src='../../../cdnjs.cloudflare.com/ajax/libs/jquery/1.12.4/jquery.minb8ff.js?ver=1.12.4' id='jquery-js'></script> <script type='text/javascript' src='../../../cdnjs.cloudflare.com/ajax/libs/highlight.js/11.2.0/highlight.minc9a3.js?ver=11.2.0' id='highlight-js'></script>            <script>hljs.highlightAll();</script> <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-1731162805004860" crossorigin="anonymous"></script></head><body class="software-template-default single single-software postid-1082512 no-sidebar"><div id="page" class="site"> <a class="skip-link screen-reader-text" href="#content">Skip to content</a><header id="topnav" class="sticky-top"><nav class="navbar navbar-expand-md navbar-light bg-light" role="navigation"> <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbar-primary" aria-controls="navbar-primary" aria-expanded="false" aria-label="Toggle navigation"> <span class="navbar-toggler-icon"></span> </button> <a class="navbar-brand" href="#">free-tor-game.com</a><div id="navbar-primary" class="collapse navbar-collapse"><ul id="menu-primary-top-menu" class="nav navbar-nav"><li itemscope="itemscope" itemtype="https://www.schema.org/SiteNavigationElement" id="menu-item-192" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-has-children dropdown menu-item-192 nav-item"><a title="Tools" href="#" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false" class="dropdown-toggle nav-link" id="menu-item-dropdown-192">Tools</a><ul class="dropdown-menu" aria-labelledby="menu-item-dropdown-192" role="menu"><li itemscope="itemscope" itemtype="https://www.schema.org/SiteNavigationElement" id="menu-item-193" class="menu-item menu-item-type-post_type menu-item-object-post menu-item-193 nav-item"><a title="ARFCN-Frequency Converter" href="../../3gpp-arfcn-frequency-converter/" class="dropdown-item">ARFCN-Frequency Converter</a></li></ul></li><li itemscope="itemscope" itemtype="https://www.schema.org/SiteNavigationElement" id="menu-item-2301" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-2301 nav-item"><a title="Contact Us" href="../../contact/" class="nav-link">Contact Us</a></li></ul></div></nav></header><div id="content" class="site-content"><div id="primary" class="content-area"><main id="main" class="site-main"><div id="main-container" class="container"><div id="main-row" class="row"><div id="main-col" class="col-12"><article id="post-1082512" class="post-1082512 software type-software status-publish hentry category-software tag-optimization"><div class="row"><div class="col-12 col-md-8"><header class="entry-header"><h1 class="entry-title text-danger">Approaching Particle Swarm Optimization with optimization problem, how to use PSO with it</h1></header><p style='font-size:1.2em'><span class="mr-2 badge badge-success">optimization</span></p><div class="entry-content"><p>I am learning Particle Swarm Optimization.</p><p>The problem is to find the pair of number whose sum is lowest.</p><p>Lets say we have some numbers z1,z2,z3,z4</p><pre><code>Number  Value
 z1     -2
 z2     -3
 z3      3
 z4     -5
</code></pre><p>The goal is to find pairs of number whose sum is minimum <strong>(z2,z4)</strong>.</p><p>Initially we have 3 available pairs to choose<br /> <strong>(z1,z2),(z2,z3),(z3,z4)</strong>.</p><p><strong>Questions</strong></p><ol><li><p>How can I formulate this as an optimization problem? I want to be able to include different combinations as well such as the combinations (x1,x2,x3,x4),(x4,x5,x6,x7) etc.</p></li><li><p>Can PSO work with this situation?</p></li></ol></div>     <div id="comments" class="comments-area"><div class="row"><div class="col-12"><div class="mt-3 border-bottom border-success"><h4 class="text-success"><i class='fa fa-check-circle text-success mr-3'></i><span>Best Answer</span></h4></div><div class='bg-transparent mb-3'><p>I took the liberty of adapting a portion of my thesis work to more clearly articulate what Particle Swarm Optimization is and how it works.</p><p>Many existing resources on PSO are not really that great or useful, in my experience. Hopefully this is more clear. It&#39;s comprehensive and has some additional references which might be helpful.</p><p>The following is the approach to take when working with optimization problems.</p><p><strong>tl;dr:</strong></p><ol><li>Understand what you are actually doing and what optimization is. You need to have context for what&#39;s going on and what you are trying to do.<ul><li>The below explains step by step, sequentially, the theory required to understand and formulate an optimization problem</li><li>You might want to skip the Kuhn-Tucker conditions section here unless you like <s>theory</s> the deep end</li><li>Several sections deal with constraints, you (or future readers) may not care.  Again, feel free to skip as needed</li></ul></li><li>Formulate your situation as an actual optimization problem<ul><li>Mathematically, not in words</li><li>Follow the sequence of this post while doing so.</li></ul></li><li>Implement PSO using your formal optimization problem</li></ol><p>In your case, the hard part is the second step (and first step). Once you can define your problem and design space, how PSO is implemented becomes a relatively trivial &#34;implementation detail&#34; as you can adapt any variant of PSO to accommodate your situation.</p><p>The trick is converting a word problem into equations, which is how this approach will let you.</p><hr/><h1>Generalized Optimization Problem Statement</h1><p>A general optimization problem contains one or more functions to be minimized or maximized. Also included are design variables and constraints. Design variables are independent parameters from which all system equations are built.  Constraints are equations or numerical routines that limit certain combinations of design variables (e.g., a prescribed cost or stress value).</p><p>The generalized form for a nonlinear constrained optimization problem is given as</p><p><a href="../../../i.stack.imgur.com/KYxZo.png" rel="nofollow noreferrer"><img src="../../../i.stack.imgur.com/KYxZo.png" alt="enter image description here"/></a></p><p>with x representing design variables for the optimization problem <sup>1</sup>.</p><p>This answer will address only problems with a single objective function (p=1).</p><h2>Design Variables</h2><p>Each optimization problem has a variety of input parameters that can be changed. These inputs are known as design variables (DVs). For example, if attempting to minimize weight for a pop can, design variables might be can height, material thickness, and can diameter.</p><h2>Objective Function</h2><p>The objective function is the function being optimized.  Its calculated value is often referred to as a fitness value and objective functions are generally represented as a function of some or all design variables.  For the pop can example, the objective function might be an expression representing the total weight of the can as a function of its height, thickness, and diameter.</p><p>An objective function can either be minimized or maximized, but convention within formal optimization is to minimize objective functions.</p><h2>Constrained Optimization</h2><p>Engineering problems often have limitations imposed on the system. These may be the result of resource limitations such as geometric constraints, physical limitations, or other sources. Continuing the example of a pop can, constraints might be a maximum total cost, a target can volume, or a target diameter to height ratio. Constraints are reflected in optimization problems through numeric equations or computational routines.</p><p>An example constrained optimization problem is shown below:</p><p><a href="../../../i.stack.imgur.com/gFSS5.png" rel="nofollow noreferrer"><img src="../../../i.stack.imgur.com/gFSS5.png" alt="enter image description here"/></a></p><p>There are three primary types of constraints within optimization problems -  inequality, equality, and side. The example contains a single inequality constraint (x<sub>1</sub> - x<sub>2</sub> -5 &lt;= 0), a single equality constraint (x<sub>1</sub>^2+x<sub>2</sub>^2 - 10 = 0), side constraints for both x<sub>1</sub> and x<sub>2</sub>, and a total of two design variables.</p><h2>Inequality Constraints</h2><p>Constraints expressed as inequalities are simply referred to as inequality constraints. The inequality constraint above would be x<sub>1</sub>+x<sub>2</sub> -5 &lt;= 0.</p><p>Inequality constraints are usually converted to be of the form g(<strong>x</strong>) &lt;= 0 and normalized. In a generalized case, inequality constraints within optimization are represented by:</p><ul><li>g<sub>j</sub>(<strong>x</strong>) &lt;= 0  j = 1,...,m</li></ul><p>where j represents each of m constraints, and <strong>x</strong> is the independent design variable vector.</p><p>When a constraint g(<strong>x</strong>) = 0, the constraint is considered to be an active constraint. When  g(<strong>x</strong>) &lt; 0, it is considered a satisfied constraint.</p><h2>Equality Constraints</h2><p>Equality constraints are  similar to  inequality constraints, except of the form h(<strong>x</strong>) = 0.</p><p>The equality constraint from above is</p><ul><li>x<sub>1</sub><sup>2</sup>+x<sub>2</sub><sup>2</sup> - 10 = 0</li></ul><p>Generally, equality constraints are represented with a small tolerance to help optimization methods obtain feasible solutions.</p><h2>Side Constraints</h2><p>Side constraints are lower and upper bounds for the independent design variables. From above, side constraints are</p><ul><li>0 &lt;= x<sub>1</sub> &lt;= 5</li><li>0 &lt;= x<sub>2</sub> &lt;= 5</li></ul><h2>Design Space</h2><p>Within a given problem, the space within the coordinate system design variables can exist is called the design space. This space is multidimensional, so as the number of design variables increases, the dimensionality of the design space also increases with an equal number of dimensions. This dimensionality is often described as n-D, with <em>n</em> representing the number of dimensions. A combination of design variables is known as a design point and reflects a location within the design space.</p><p>The design space refers to the numerical domain where the design variables, objective function, and constraints all coincide.  A design space has a dimensionality equal to the number of design variables.  A  design point is a specific combination of design variables in the design space.  Using the values at a design point, outputs for the objective function and constraints can be computed.</p><p>An example design space is shown in Figure 1.1 (Kalivarapu<sup>8</sup>) with 1 design variable, c. The x-axis represents the design space while the y-axis represents the objective function value.</p><p><a href="../../../i.stack.imgur.com/5xueu.png" rel="nofollow noreferrer"><img src="../../../i.stack.imgur.com/5xueu.png" alt="enter image description here"/></a></p><p>A 2-D problem can be thought to have a design space occupying the XY plane. The Z-axis represents the objective function value for each point in the XY plane.  The objective function is represented by contours for arbitrary objective function values plotted on the XY plane. Inequality constraints are plotted in the XY plane and represent boundaries where the constraint equals zero.</p><p>A 3-D problem would have its design space as the XYZ volume. A design space for pop can design might be different combinations of height, material thickness, and can diameter.</p><p>Problems with higher dimensionality cannot be directly visualized.</p><h1>Solving Constrained Optimization Problems</h1><h2>Kuhn-Tucker Conditions</h2><p>(Skip this section unless you really, <em>really</em> want to understand constrained optimization)</p><p>An important requirement for solving constrained optimization problems are Kuhn-Tucker conditions. A solution must satisfy the three conditions described by Kuhn-Tucker in the 1950s to represent a local optimum for a constrained optimization problem <sup>2</sup>. The three conditions are described below.</p><p><a href="../../../i.stack.imgur.com/RKpdu.png" rel="nofollow noreferrer"><img src="../../../i.stack.imgur.com/RKpdu.png" alt="enter image description here"/></a></p><p>The first Kuhn-Tucker condition is shown in Equation 1.3 and is simply a requirement stating a design point representing a local optimum must satisfy all constraints.</p><p>Next, the product of the Lagrange multipler (lambda<sub>j</sub>) and constraint value g(<strong>x</strong>) must be equal to zero as shown in Equation 1.4. A Lagrange multiplier reflects the rate of change of an objective function with respect to rate of change in a constraint. Lagrange multipliers (lambda<sub>j</sub>$) are equal to zero for satisfied and non-active constraints. When a constraint is active, the value of the constraint g(<strong>x</strong>) is equal to zero. When all constraints are either satisfied or active, the second Kuhn-Tucker condition represented in Equation 1.3 is equal to zero.</p><p><a href="../../../i.stack.imgur.com/IwNN0.png" rel="nofollow noreferrer"><img src="../../../i.stack.imgur.com/IwNN0.png" alt="enter image description here"/></a></p><p>Finally, Figure 1.3 shows the region where an example design point can move to improve as the Usable-Feasible sector. This is the direction in design space which decreases the objective function and remains feasible. At optimality, the direction for improved search represented by Delta F(<strong>x</strong>) will be perpendicular to all active constraints. This is the third Kuhn-Tucker condition and represented in Equation 1.5.  Satisfied constraints are not included as their Lagrange multiplier is 0 as discussed previously.</p><p>Kuhn-Tucker conditions do not guarantee a global optimum unless a design space is convex but are used by some methods to determine whether a point is a local minimum.</p><h1>Sequential Unconstrained Minimization Techniques</h1><p>(Skip this section unless you really, <em>really</em> want to understand constrained optimization)</p><p>Sequential Unconstrained Minimization Techniques (SUMT) are methods to convert a constrained optimization problem into an unconstrained problem. This is generally done by adding a penalty for design points that violate constraints. This penalty results in a new objective function as shown in Equation 1.6 below. This is sometimes referred to as a pseudo objective function. P(<strong>x</strong>) represents a penalty component added to the objective function based on constraint violations. This is shown in Equation 1.6.</p><p><a href="../../../i.stack.imgur.com/D1kEE.png" rel="nofollow noreferrer"><img src="../../../i.stack.imgur.com/D1kEE.png" alt="enter image description here"/></a></p><p>where F(<strong>x</strong>) is the original objective function, P(<strong>x</strong>) represents the penalty function, and r<sub>p</sub> is a scalar multiplier that is sometimes used by the penalty function. The scalar r<sub>p</sub> is generally set initially and changed throughout subsequent optimization runs to improve solution characteristics.</p><p>SUMT methods result in the solution of several unconstrained minimization problems in sequence by adjusting the penalty term until convergence is achieved.</p><h2>Penalty Functions</h2><p>(Skip this section unless you really, <em>really</em> want to understand constrained optimization)</p><p>Many of the SUMT algorithms are commonly known as penalty functions because they incorporate a numeric penalty when constraints are violated. Adding a penalty to the objective function for infeasible points causes infeasible points to be less desirable as this increases the pseudo-objective function value.</p><p>Two of the most common ways to calculate P(<strong>x</strong>) are the Exterior Penalty Function (EPF) and Interior Penalty Function (IPF). EPF adds a penalty term based on the sum of squares of constraint violations. IPF provides a penalty based on the inverse of the constraint value. Both of these are discussed in more details by Vanderplaats<sup>1</sup>. Because the way a penalty is imposed can lead to numerically ill-conditioned problems, care must be taken when applying a penalty factor to avoid divergence.</p><p>When using penalty functions, constraint violations are used to create an additional term which is added to the term being optimized. Consequentially this allows otherwise unconstrained optimization techniques to be use for constrained problems.</p><h2>Direct Methods</h2><p>(Skip this section unless you really, <em>really</em> want to understand constrained optimization)</p><p>While the previous section discussed modifications to problem formulation allowing unconstrained methods to be applied to problems containing constraints, another class of methods deals directly with constraints. Direct methods tend to be efficient and offer improved precision but generally require solutions to remain feasible and both constraints and objective functions must be composed of differentiable functions <sup>1</sup>.</p><p>One of these methods is the Method of Feasible Directions. Heavily relying on gradients, this method identifies a region within the design space a point can move to reduce the objective function and remain feasible <sup>1</sup>. Iterations progress by forcing the design point to consistently decrease in fitness while remaining feasible. This is shown in Figure 1.4 (Vanderplaats) The usable-feasible sector represents the area the current design point can move and simultaneously decrease the objective function and satisfy constraints.</p><p><a href="../../../i.stack.imgur.com/NbTSX.png" rel="nofollow noreferrer"><img src="../../../i.stack.imgur.com/NbTSX.png" alt="enter image description here"/></a></p><p>The Generalized Reduced Gradient method effectively converts all constraints into equality constraints by adding variables called slack variables to all inequality constraints. It then uses gradients to calculate a reduced gradient and performs a 1-D search (see Vanderplaats for more information). The basic search logic for the General Reduced Gradient is shown in Figure 1.5.</p><p><a href="../../../i.stack.imgur.com/TrZp4.png" rel="nofollow noreferrer"><img src="../../../i.stack.imgur.com/TrZp4.png" alt="enter image description here"/></a></p><h1>Particle Swarm Optimization</h1><p>Particle swarm optimization (or PSO) is a heuristic based method developed in 1995 in order to solve optimization problems <sup>3</sup>. The PSO method was developed with inspiration from the social and nesting behaviors exhibited in nature (e.g., of flocks of birds, schools of fish, and swarming insects) <sup>4</sup>. It uses an array of mathematical particles simultaneously examining a mathematical space searching for improved locations. This search is analogous to the search for food or nesting locations within nature. PSO is designed to solve unconstrained optimization problems.</p><p>PSO contains multiple design points simultaneously. These are called particles and individually explore the design space. Typically there are 10 times as many particles as design variables. Collectively, particles are referred to as a swarm and each possess the following attributes:</p><ul><li><strong>Design variables</strong> - current location of the particle represented by design variables values</li><li><strong>Fitness</strong> -  objective function value at the current location</li><li><strong>Velocity</strong> - speed the particle is moving in each direction, with a velocity component for each design variable dimension</li><li><strong>pBest</strong> - design point representing the best design point in the particle&#39;s history as it has moved in the design space</li><li><strong>gBest</strong> -  current best particle in the swarm, evaluated by fitness value</li></ul><p>The basic format for each PSO iteration is shown in Figure 1.6. After random initialization, particles first move to different design points within the design space. Once particles move, fitness is evaluated and pBest each updated. The swarm gBest is then updated and each particle velocity calculated for the next iteration. Position and velocity are updated using update Equation 1.7 through Equation 1.9.</p><p><a href="../../../i.stack.imgur.com/YLX8H.png" rel="nofollow noreferrer"><img src="../../../i.stack.imgur.com/YLX8H.png" alt="enter image description here"/></a></p><p>Equation 1.7 is the velocity update equation. This determines the new velocity for each particle <strong>i</strong> for the next iteration (<strong>v</strong><sub>iter+1, i</sub>). The current velocity (<strong>v</strong><sub>iter, i</sub>),  current particle location (<strong>x</strong><sub>i</sub>), and current gBest and pBest locations (x<sub>gBest</sub> and x<sub>pBest</sub>)  are used to calculate the new velocity. Parameters c<sub>1</sub> and c<sub>2</sub> are user defined and typically set to 2.0 <sup>5</sup>. The particle moves each iteration according to Equation 1.8.</p><p><a href="../../../i.stack.imgur.com/Ev1e3.png" rel="nofollow noreferrer"><img src="../../../i.stack.imgur.com/Ev1e3.png" alt="enter image description here"/></a></p><p>Figure 1.7 (Kalivarapu<sup>8</sup>) shows how the components of the velocity update affect the particle&#39;s location each iteration.</p><p><a href="../../../i.stack.imgur.com/2pIif.png" rel="nofollow noreferrer"><img src="../../../i.stack.imgur.com/2pIif.png" alt="enter image description here"/></a></p><p>In order to facilitate convergence and better allow exploration during initial iterations, Equation 1.9 represents an inertial weight affecting velocity (w<sub>iter</sub>). As iterations increase, the effect of lambda<sub>w</sub> causes velocity to be slowly damped throughout iterations as it is typically a value slightly below 1.0 <sup>6</sup>. Additional discussion on the inertial weight factor can be found in <sup>7</sup>.</p><p>Convergence criteria are user defined and checked each iteration. An example is having 50 sequential iterations with no more than 0.0001 improvement in gBest.</p><h2>References</h2><p><sub>1:  Garret N Vanderplaats. “Multidiscipline design optimization.” Vanderplaats Research &amp; Development, Incorporated, 2007.  </sub></p><p><sub>2: Harold W Kuhn and Albert W Tucker. “Nonlinear programming.” In Proceedings of the second Berkeley symposium on mathematical statistics and probability, volume 5. California, 1951.  </sub></p><p><sub>3: Russell Eberhart and James Kennedy. “A new optimizer using particle swarm theory.” In  Micro Machine and Human Science, 1995. MHS’95., Proceedings of the Sixth International Symposium on, pages 39–43. IEEE, 1995.</sub></p><p><sub>4: Craig W. Reynolds. “Flocks, herds and schools: A distributed behavioral model.” ACM SIGGRAPH Computer Graphics, 21(4):25–34, August 1987.  </sub></p><p><sub>5: Yuhui Shi and R Eberhart. “Parameter selection in particle swarm optimization.” Evolutionary Programming VII, pages 1–12, 1998.</sub></p><p><sub>6: Y. Shi and R. Eberhart. “A modified particle swarm optimizer.” 1998 IEEE International Conference on Evolutionary Computation Proceedings. IEEE World Congress on Computational Intelligence (Cat. No.98TH8360), pages 69–73. </sub></p><p><sub>7: R.C. Eberhart and Y. Shi. “Comparing inertia weights and constriction factors in particle swarm optimization.” Proceedings of the 2000 Congress on Evolutionary Computation, 1(7):84–88, 2000. </sub></p><p><sub>8: Vijay Kiran Kalivarapu. “Improving solution characteristics of particle swarm optimization through the use of digital pheromones, parallelization, and graphical processing units (GPUs).” Iowa State University, 2008.   </sub></p></div></div><div class="col-4"></div></div></div></div><div class="col-12 col-md-4"> <ins class="adsbygoogle"
 style="display:block"
 data-ad-client="ca-pub-1731162805004860"
 data-ad-slot="1340983829"
 data-ad-format="auto"
 data-full-width-responsive="true"></ins>   <div class='mt-3 ml-4 border-bottom border-success'><h6><span>Related Question</span></h6></div><ul class='list-group list-group-flush'><li class="list-group-item"><a href='../c-do-i-get-valid-profiling-results-with-optimization-turned-off/'>C++ &#8211; Do I get valid profiling results with optimization turned off</a></li></ul></div></div><footer class="entry-footer"></footer></article></div></div></div></main></div></div><footer id="colophon" class="site-footer"><div class="site-info"></div></footer></div>    <script type='text/javascript' src='../../../cdn.jsdelivr.net/npm/mathjax%403/es5/tex-chtml40df.js?ver=5.6' id='mathjax-js'></script> <script type='text/javascript' src='../../../cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/esm/popper.min43d3.js?ver=1.14.7' id='popper-js'></script> <script type='text/javascript' src='../../../cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.3.1/js/bootstrap.min5b31.js?ver=4.3.1' id='bootstrap4-js'></script> <script type='text/javascript' id='itectec-common-js-extra'>var globalObject = {"homeUrl":"https:\/\/free-tor-game.com\/"};</script> <script defer src="../../wp-content/cache/autoptimize/4/js/autoptimize_63b39f43631847816e6b0e70ea726624.js"></script></body></html>